{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "film_script_generation_gtp2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1EYnGMObGkAEitlKNQrf2Zp07Axl7nPXX",
      "authorship_tag": "ABX9TyN8OkbH5VZCRVQ43caXpcTh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bcd1c0f58bb14eac924f2e5d12ec71aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8c70a36c82d34d2596e20e5f7703608c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_72c6a05cb7934e579fa7882ca3d86568",
              "IPY_MODEL_7cb285719a4d43e39d4d33cce6c6e4c6"
            ]
          }
        },
        "8c70a36c82d34d2596e20e5f7703608c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72c6a05cb7934e579fa7882ca3d86568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5f49f2db18b54edc8729783bf999a44c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_33370ba9973542049d45d707982f21e7"
          }
        },
        "7cb285719a4d43e39d4d33cce6c6e4c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a39dd02f8954484ba0b95c5eb2da67b2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 1.45MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e1c14a1a8cb48dfae90ca52e55c3dd3"
          }
        },
        "5f49f2db18b54edc8729783bf999a44c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "33370ba9973542049d45d707982f21e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a39dd02f8954484ba0b95c5eb2da67b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e1c14a1a8cb48dfae90ca52e55c3dd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c4810bc44f64d4f94bb4f932a3ef96c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1898b954bac140ed83e061cb9d2aea0a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6b523e44f42747e68a24bb26e5f98aee",
              "IPY_MODEL_a5eb94c60c6d43be8e90d3070135c430"
            ]
          }
        },
        "1898b954bac140ed83e061cb9d2aea0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b523e44f42747e68a24bb26e5f98aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_61a38f50391c485384092bc114bd2999",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fb6ed76f4bb84007b14c24372009a2ff"
          }
        },
        "a5eb94c60c6d43be8e90d3070135c430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c021b62cf5cf4d98b1e2e24bd73d1b92",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 2.18MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_871d9b5142b44d7ead09c0609d6bb3cd"
          }
        },
        "61a38f50391c485384092bc114bd2999": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fb6ed76f4bb84007b14c24372009a2ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c021b62cf5cf4d98b1e2e24bd73d1b92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "871d9b5142b44d7ead09c0609d6bb3cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e2843a7fb244e09bb64281e88dba08b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d912bf621cfe4425b255a5d834ea6a8f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8e7585ccbd0345acb12db2d2b8f94635",
              "IPY_MODEL_da39cb91a9884ee9a887964bb3a2f899"
            ]
          }
        },
        "d912bf621cfe4425b255a5d834ea6a8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8e7585ccbd0345acb12db2d2b8f94635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_200508136c3942f89e4de891d987bd82",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 665,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 665,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_724fa87fee6b43aab0049994256c7552"
          }
        },
        "da39cb91a9884ee9a887964bb3a2f899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f4c92201df1e401f81b6044e69c9d569",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 665/665 [00:00&lt;00:00, 829B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b26463babfe74b89aa4fabf9d47d8a19"
          }
        },
        "200508136c3942f89e4de891d987bd82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "724fa87fee6b43aab0049994256c7552": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f4c92201df1e401f81b6044e69c9d569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b26463babfe74b89aa4fabf9d47d8a19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ba1b1c01ef24a51a1c5cca9486f7650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_094fd194afde49f18257462bbf944f11",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_19d920c34a9a4313b9154b88c51058e4",
              "IPY_MODEL_eca974f3811947768ca6b6e3c7ea921f"
            ]
          }
        },
        "094fd194afde49f18257462bbf944f11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "19d920c34a9a4313b9154b88c51058e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8c122422cfa44c988ce8c62b90bd706d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 548118077,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 548118077,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_59342db9dd0e4ddbae78c58d76023933"
          }
        },
        "eca974f3811947768ca6b6e3c7ea921f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c7f41a087702425a9c7d0b4d401588c4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [00:50&lt;00:00, 10.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_25da1b2051374634afd210e918fa91f3"
          }
        },
        "8c122422cfa44c988ce8c62b90bd706d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "59342db9dd0e4ddbae78c58d76023933": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7f41a087702425a9c7d0b4d401588c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "25da1b2051374634afd210e918fa91f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizarci3/gtp2_film_generation/blob/master/film_script_generation_gtp2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zngvnu2UwJbf",
        "colab_type": "text"
      },
      "source": [
        "# Background\n",
        "\n",
        "Original article can be found [here](https://towardsdatascience.com/film-script-generation-with-gpt-2-58601b00d371)\n",
        "\n",
        "Repo [here](https://github.com/cdpierse/script_buddy_v2)\n",
        "\n",
        "The author used film scripts (~60 MB) of data scraped from the Internet Movie Script Database (IMSDB) in order to fine-tune a GTP2 to write a film script.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_StsRw6xxYJ2",
        "colab_type": "text"
      },
      "source": [
        "The author of this script only had ~1300 scripts to use, however, on averagea screenplay has 30,000 words. So the dataset has close to 40 million sequences of words.\n",
        "\n",
        "The author wanted the model to be able to generate entire sequences of scripts with mixed scripted elements in each sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x3MxXos4Z6c",
        "colab_type": "text"
      },
      "source": [
        "This fine-tuning is developed based on hugginface's example on fine-tuning dataset found in run_language_modeling.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Oj1u6_enWP1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "6db58571-86a0-4eba-f76c-bb65d9dda9c6"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=b4e81acce39573b28065401055e044329b17124bf4abea42cff3b4f6eaa2ec12\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.8 GB  |     Proc size: 112.1 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total     11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkEM-rEJ-xrA",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the data\n",
        "\n",
        "The script data is loaded into the model in batches were the data has already been tokenized for GPT-2. In the repo, the ScriptData class splits the entire dataset into tokenized blocks of tensors.\n",
        "\n",
        "Once the data has been properly prepared, these blocks are loaded in batches into a GPT-2 in a training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r-ZwZkNMMPf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "e1368c97-61b4-45f4-e7dd-eab048d5f09d"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers #just doing a pip install transformers creates some sync problems"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-q1iybb3n\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-q1iybb3n\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.18.5)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 42.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 25.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.6.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (0.16.0)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-3.0.2-cp36-none-any.whl size=824179 sha256=9a8983b5b5bae24172e9ebabc03d770f81a4870b6ba4e63ea7358444865f4f11\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wud6vgr6/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=3d330a0478b8e930ccd1079d2d251c4a7daa2dff18d6483174ebf0936384d873\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFh74RApLrem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import os\n",
        "import logging \n",
        "import pickle\n",
        "import logging\n",
        "import random"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBQQEEeTFt7v",
        "colab_type": "text"
      },
      "source": [
        "The gpt2-medium model used in this work has 12 layers ~345 million parameters and took ~6h to train (with 3 epochs).\n",
        "\n",
        "The first thing that needs to be done is to upload the model and tokenizer from the pre-trained transformers package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VxKJlVLf5zs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a0eed238-2b70-4b7f-bd07-383540256f54"
      },
      "source": [
        "device = 'cpu'\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "\n",
        "device"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R2k5VtgFa-m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bcd1c0f58bb14eac924f2e5d12ec71aa",
            "8c70a36c82d34d2596e20e5f7703608c",
            "72c6a05cb7934e579fa7882ca3d86568",
            "7cb285719a4d43e39d4d33cce6c6e4c6",
            "5f49f2db18b54edc8729783bf999a44c",
            "33370ba9973542049d45d707982f21e7",
            "a39dd02f8954484ba0b95c5eb2da67b2",
            "6e1c14a1a8cb48dfae90ca52e55c3dd3",
            "5c4810bc44f64d4f94bb4f932a3ef96c",
            "1898b954bac140ed83e061cb9d2aea0a",
            "6b523e44f42747e68a24bb26e5f98aee",
            "a5eb94c60c6d43be8e90d3070135c430",
            "61a38f50391c485384092bc114bd2999",
            "fb6ed76f4bb84007b14c24372009a2ff",
            "c021b62cf5cf4d98b1e2e24bd73d1b92",
            "871d9b5142b44d7ead09c0609d6bb3cd",
            "9e2843a7fb244e09bb64281e88dba08b",
            "d912bf621cfe4425b255a5d834ea6a8f",
            "8e7585ccbd0345acb12db2d2b8f94635",
            "da39cb91a9884ee9a887964bb3a2f899",
            "200508136c3942f89e4de891d987bd82",
            "724fa87fee6b43aab0049994256c7552",
            "f4c92201df1e401f81b6044e69c9d569",
            "b26463babfe74b89aa4fabf9d47d8a19",
            "3ba1b1c01ef24a51a1c5cca9486f7650",
            "094fd194afde49f18257462bbf944f11",
            "19d920c34a9a4313b9154b88c51058e4",
            "eca974f3811947768ca6b6e3c7ea921f",
            "8c122422cfa44c988ce8c62b90bd706d",
            "59342db9dd0e4ddbae78c58d76023933",
            "c7f41a087702425a9c7d0b4d401588c4",
            "25da1b2051374634afd210e918fa91f3"
          ]
        },
        "outputId": "7df2eaa8-f3de-4b20-894c-86e862f3f59e"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.to(device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcd1c0f58bb14eac924f2e5d12ec71aa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descriptâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c4810bc44f64d4f94bb4f932a3ef96c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e2843a7fb244e09bb64281e88dba08b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ba1b1c01ef24a51a1c5cca9486f7650",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KINDLD6oGjr4",
        "colab_type": "text"
      },
      "source": [
        "* In order to fine tune this pre-trained model you need to create a training loop where you progressively load a batch of script sequences from the entire dataset.\n",
        "* Each batch is like a tokenized block of tensors from the data (done in ScriptData).\n",
        "* An important parameter to consider is the batch size. Large batch sizes can result in running out of GPU memory fast. To start, you can choose a batch of 1 and then test how much you can test it.\n",
        "\n",
        "* In this work his batch size was 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heJFj7VS_V-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from transformers import (\n",
        "    GPT2Config,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        ")\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7W7V0_cZciV",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the Data\n",
        "\n",
        "The following ScriptData class splits the dataset into tokenized blocks of tensors. These blocks will then be loaded in batches into the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzBvVN9g3wgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FILE_PATH = \"/content/drive/My Drive/WJ/film_text.txt\" # ~ 60 MB\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ScriptData(Dataset):\n",
        "\n",
        "  def __init__(\n",
        "      self, #instance of the class ScriptData\n",
        "      tokenizer: PreTrainedTokenizer,\n",
        "      file_path: str, \n",
        "      block_size = 512, # Fine-tuning item\n",
        "      overwrite_cache = False\n",
        "  ):\n",
        "\n",
        "      assert os.path.isfile(file_path) #assert raises an error if condition False\n",
        "\n",
        "      block_size = block_size - (tokenizer.max_len - tokenizer.max_len_single_sentence)\n",
        "\n",
        "      directory, filename = os.path.split(file_path)\n",
        "\n",
        "      #Create the path/filename for the cached file\n",
        "      # so that is stored in the same folder and it stores which block size we used\n",
        "    \n",
        "      cached_features_file = os.path.join(directory,\"gpt2\"+\"_\"+str(block_size)+\"_\"+filename)\n",
        "\n",
        "      #if the file already exists and if overwrite_cache is set to False don't overwrite\n",
        "      if os.path.exists(cached_features_file) and not overwrite_cache:\n",
        "        logger.info(f\"Loading features from cached file {cached_features_file}\")\n",
        "\n",
        "        with open(cached_features_file, 'rb') as cache:\n",
        "          self.examples = pickle.load(cache)\n",
        "          logger.debug(\"Loaded examples from cache\")\n",
        "\n",
        "      else:\n",
        "\n",
        "        logger.info(f\"Creating features from file {filename} at {directory}\")\n",
        "\n",
        "        self.examples = []\n",
        "\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "          text = f.read()\n",
        "          logger.debug(\"Succesfully read text from file\")\n",
        "\n",
        "        #convert_tokens_to_ids = Converts a token string (or a sequence of tokens) in a single integer id (or sequence of ids), using the vocabulary\n",
        "        tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
        "\n",
        "        #slice in steps of block_size the text\n",
        "        #append in examples\n",
        "\n",
        "        for i in range(0, len(tokenized_text) - block_size + 1, block_size):\n",
        "          self.examples.append(\n",
        "              tokenizer.build_inputs_with_special_tokens( #From Bert model: \n",
        "                  tokenized_text[i : i + block_size]\n",
        "              )\n",
        "          )\n",
        "\n",
        "        logger.info(f\"Saving features into cached file {cached_features_file}\")\n",
        "\n",
        "        # save it\n",
        "        with open(cached_features_file, \"wb\") as cache:\n",
        "          pickle.dump(self.examples, cache, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return torch.tensor(self.examples[item], dtype=torch.long)\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap9UBZprEuWu",
        "colab_type": "text"
      },
      "source": [
        "# Fine-tuning GPT-2: Training\n",
        "\n",
        "A GPU is necessary when training this model. We are using a dataset of film scripts that is about 60 MB to train. This text has been prepared by scrapping IMSDB (see in the repo the specifics of the scrapping)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAJXisafcko_",
        "colab_type": "text"
      },
      "source": [
        "We talked about how to fine tune the model (or optimize it on a custom dataset of tokenized text) you need to create a TRAINING LOOP WHERE YOU PROGRESSIVELY LOAD A BATCH OF SCRIPT SEQUENCES FROM THE DATASET.\n",
        "\n",
        "* Each batch (a batch of tokenized tensor) is run through the language model head as BOTH its intput and target labels.\n",
        "* From this step we return the loss and logits (i.e., prediction scores) to conduct the backward pass on the gradients.\n",
        "* Every X number of batches set up an evaluation step to generate a batch of text. This helps us understand how well the model is optimizing and being fine-tuned to the specific text.\n",
        "* Transformer's generate function provides a number of different decoding methods to get the best results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NCDHbQUZFnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_dir = '/content/drive/My Drive/WJ/'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbX0rIZoeMwi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "4afb48fa-85f5-48eb-f29a-d8791cfd30f2"
      },
      "source": [
        "dataset = ScriptData(tokenizer= tokenizer, file_path= FILE_PATH )\n",
        "script_loader = DataLoader(dataset,batch_size=4,shuffle=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1294: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2QWPMoibZ4y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b6cb133-5244-4ef8-e6f6-6523d9bebf59"
      },
      "source": [
        "type(script_loader)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL3xEeRDZnz2",
        "colab_type": "text"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PufIi3UYUdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 1 #starting point, author used in the end a batch_size of 7\n",
        "EPOCHS = 1 # the author mentions he used in total 3 full epochs lasting ~6h for training\n",
        "LEARNING_RATE = 0.00002\n",
        "WARMUP_STEPS = 10000"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTcMfEkAZ2MS",
        "colab_type": "text"
      },
      "source": [
        "Start the optimizer, scheduler and set up the loss, batch counts to start at zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io3W0TKaYUgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.train()\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfGVZd6ZTLU0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "02e3ff40-4902-4ad3-fd62-311b0f7db0ff"
      },
      "source": [
        "device"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKXA1ZOaUBwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_perplexity(encoded_joke, input_stride):\n",
        "  lls = []\n",
        "\n",
        "\n",
        "  for i in range(1, encoded_joke.size(1), input_stride):\n",
        "    begin_loc = max(i + input_stride - encoded_joke.size(1), 0)\n",
        "    end_loc = i + input_stride\n",
        "\n",
        "    input_ids = encoded_joke[:,begin_loc:end_loc].to(device)\n",
        "\n",
        "    target_ids = input_ids.clone()\n",
        "    target_ids[:,:-input_stride] = -100\n",
        "\n",
        "    with torch.no_grad():\n",
        "      ppl_output = model(input_ids, labels=target_ids)\n",
        "      log_likelihood = ppl_output[0]*input_stride\n",
        "\n",
        "    lls.append(log_likelihood)\n",
        "\n",
        "  perplex = torch.exp(torch.stack(lls).sum()/i)\n",
        "\n",
        "  return(perplex.item())"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjD7JoJ87jbt",
        "colab_type": "text"
      },
      "source": [
        "## Just for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE1xdpHCYUko",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "567f997a-ff5e-4ad7-cc19-dda91ef51162"
      },
      "source": [
        "script_count = 0\n",
        "sum_loss = 0.0\n",
        "batch_count = 0\n",
        "\n",
        "# input text: You can use input_ids or bos_token_id to start your text generation\n",
        "# bos_token_id should be 1 positive int (in the setup we have below, it initializes with a random word)\n",
        "# in model generate choose how you want your text to begin\n",
        "\n",
        "test_text = \"Once upon a time there was a dog\"\n",
        "tokenized_test = torch.tensor(tokenizer.encode(test_text)).unsqueeze(0).to(device)\n",
        "\n",
        "# or\n",
        "bos_token_id_gen = random.randint(1,30000) \n",
        "\n",
        "\n",
        "# for perplexity\n",
        "stride = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"EPOCH {epoch} started\"+'='*30)\n",
        "\n",
        "  j = 0\n",
        "  nsamples = 199\n",
        "\n",
        "  for idx, script in enumerate(script_loader):\n",
        "\n",
        "    if j>nsamples:\n",
        "      break\n",
        "\n",
        "    else:\n",
        "      #print(idx, script, script[0].shape) #512 was what we used as block size in ScriptData\n",
        "\n",
        "      outputs = model(script.to(device), labels=script.to(device))\n",
        "\n",
        "      loss, logits = outputs[:2] # language modeling loss and prediction scores of the language modeling head \n",
        "      loss.backward()\n",
        "\n",
        "      sum_loss = sum_loss + loss.detach().data\n",
        "      script_count = script_count + 1\n",
        "      #print('Sum loss', sum_loss, 'script_count', script_count)\n",
        "      \n",
        "      #once we have loaded enough scripts == batch_size\n",
        "\n",
        "      j = j + 1 \n",
        "\n",
        "      if script_count == BATCH_SIZE:\n",
        "        #print('script_count equal to batch_size')\n",
        "        script_count = 0 # re-start script counter\n",
        "        batch_count = batch_count + 1 # how many full batches we have\n",
        "        #print('batch_count', batch_count)\n",
        "        \n",
        "        optimizer.step() # perform an optimization step\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad() # clear the gradients from the last step, PyTorch accumulates the gradients so before starting to propagate we need to set them to zero\n",
        "        model.zero_grad()\n",
        "\n",
        "\n",
        "        ## As opposed to the jokes generation script, this script generates text\n",
        "        ## Apparently, every X number of batches to see how it is doing\n",
        "       \n",
        "        if batch_count == 200:\n",
        "            model.eval()  \n",
        "            print(f\"sum loss {sum_loss}\")\n",
        "\n",
        "            ## see https://huggingface.co/blog/how-to-generate\n",
        "           \n",
        "            sample_outputs = model.generate( #function added since version 2.4\n",
        "                                    #bos_token_id=bos_token_id_gen, # Beginning of sentence token if no prompt is provided. The sequence used as a prompt for the generation. If `None` the method initializes  it as an empty `torch.LongTensor` of shape `(1,)`. \n",
        "                                    input_ids = tokenized_test, #(optional) tf.Tensor of dtype=tf.int32 of shape (batch_size, sequence_length) The sequence used as a prompt for the generation. If None the method initializes it as an empty tf.Tensor of shape (1,).\n",
        "                                    do_sample=True, #If set to `False` greedy decoding is used. Otherwise sampling is used. In its basic form, in sampling you randomly choose the next token  \n",
        "                                    top_k=50, #HERE The number of highest probability vocabulary tokens to keep for top-k-filtering. Must be between 0 and 1. Default to 50.  \n",
        "                                    max_length = 1000, # The max length of the sequence to be generated.  Between `min_length` and infinity. Default to 20.\n",
        "                                    top_p=0.95, #The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.  \n",
        "                                    num_return_sequences=1 #The number of independently computed returned sequences. If you want to choose between different options set > 1. Default to 1.\n",
        "                                )\n",
        "\n",
        "            print(\"Output:\\n\" + 100 * '-')\n",
        "            #print('bos_token_id_gen', bos_token_id_gen)\n",
        "            for i, sample_output in enumerate(sample_outputs):\n",
        "\n",
        "                  ppl = calculate_perplexity(sample_outputs, stride)\n",
        "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "                  print('Perplexity', ppl)\n",
        "            \n",
        "            batch_count = 0\n",
        "            sum_loss = 0.0\n",
        "            model.train()\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 0 started==============================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "sum loss 316.6806640625\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: Once upon a time there was a dog, there was an ewes.\n",
            "\n",
            "Then a young male leper stepped into the small space, then a leopard.\n",
            "\n",
            "After that, the beast's back began to creep away.\n",
            "\n",
            "\"Where are they now?\" my sister asked me, still looking at him.\n",
            "\n",
            "\"I think the house has been broken into,\" he answered, looking at me with a pale expression.\n",
            "\n",
            "After that he stopped and I asked the young man if he had seen them. \"They were still down there,\" he said, smiling.\n",
            "\n",
            "I looked at him sadly. \"So we can get out?\" I asked. \"It's too dangerous, man.\"\n",
            "\n",
            "He didn't seem to know what to say to me.\n",
            "\n",
            "\"I see you could,\" the young man said, and began to walk away. Then, he stopped to give me a pat on the shoulder.\n",
            "\n",
            "\"Goodbye, sir,\" I said, and looked down into the empty home where I had just been.\n",
            "\n",
            "My sister went to find me and saw the young man walking away from me. He was sitting in the middle of the house.\n",
            "\n",
            "\"I'm sorry, I don't understand,\" the young man said.\n",
            "\n",
            "\"I think he must have thought he was an animal that had escaped his home, he's trying to find food. Perhaps he was looking to get away in this house and try to kill us.\"\n",
            "\n",
            "\"It's a cruel story,\" the young man said, and disappeared.\n",
            "\n",
            "The young man did not look at me and he was not looking away again.\n",
            "\n",
            "The next day the little leper left, walking across the river.\n",
            "\n",
            "The young man came again in a single canoe along the shore, in the same canoe, and said to me:\n",
            "\n",
            "\"What does the river have to do with it, then?\"\n",
            "\n",
            "\"There's a great deal of water, too. You'll catch some fish. Come along.\"\n",
            "\n",
            "\"No, no, you don't think so,\" I said.\n",
            "\n",
            "He did not go along and he did not swim at all, but I saw him going on about his adventures.\n",
            "\n",
            "He began to talk to me a little, as if he wanted to get back home.\n",
            "\n",
            "I did not understand what to think of the strange little young man's story.\n",
            "\n",
            "Then suddenly my sister asked: \"Are you still alive?\"\n",
            "\n",
            "\"Yes, but I'm just glad you don't look a lot like me.\"\n",
            "\n",
            "When she heard him say this, she said:\n",
            "\n",
            "\"What is your name, if you can speak? I don't mean to look like myself anymore. You must have got a good life.\"\n",
            "\n",
            "At that moment my sister suddenly felt the weight of fear, and she ran out of the house with a cry of despair.\n",
            "\n",
            "I was so sorry. I had never seen such an expression in my life. But now I heard the old man's voice.\n",
            "\n",
            "\"It's all right,\" he said. \"Just let me go.\"\n",
            "\n",
            "My sister followed him down the river and found him there too.\n",
            "\n",
            "And I had the greatest joy about that whole thing. I saw a little baby, a white-haired woman in a long white dress standing alone in the water. She was carrying her two little kids: the little girl had been a little bit sick and her husband was a little bad-tempered, and her baby, an old woman of only three or four months, was still very much in danger of drowning.\n",
            "\n",
            "My sister sat down in the canoe and watched her child again, all looking away from the river.\n",
            "\n",
            "She walked across the river, looking out on the land. It was like seeing her face.\n",
            "\n",
            "\"Yes, I remember you, dear girl. I thought I heard you talking. Your life is so strange.\"\n",
            "\n",
            "Then she turned to the child and said:\n",
            "\n",
            "\"What do you mean, you know, you can't look at me? I don't see any signs of you.\"\n",
            "\n",
            "\"I don't like me,\" I said, \"but I see you.\"\n",
            "\n",
            "\"No, you see me now. I was watching you. I saw you crying, and I think this is very funny because I've never been so much as a part of your life. That is so strange. You don't even know it. They said it was all for a little girl, and it's all my fault.\"\n",
            "\n",
            "Then she turned again to the child.\n",
            "\n",
            "\"Now there's a boy named Peter, and you'll tell him. He's a little boy, but he doesn't even know what a man is. If he knew the words, he would have gone to bed, and no one would have\n",
            "Perplexity 10.365653991699219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eedk2u18bFxq",
        "colab_type": "text"
      },
      "source": [
        "# Breaking down preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pui05vrJa0Tn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FILE_PATH = \"/content/drive/My Drive/WJ/film_text.txt\" # ~ 60 MB\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
        "\n",
        "model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qEsd7OnaxhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert os.path.isfile(FILE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55UX9iYRYOfB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3b61922a-463c-48a3-de45-0d0ea34f921d"
      },
      "source": [
        "tokenizer.max_len, tokenizer.max_len_single_sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1024, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFZOxrpQdBag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3d10c416-b11a-42bc-8041-a250da8f924d"
      },
      "source": [
        "block_size = 512 # I am assuming this is the size of the block that will be loaded into the training loop\n",
        "\n",
        "print('tokenizer max len', tokenizer.max_len, 'tokenizer max len single sentence')\n",
        "\n",
        "block_size = block_size - (tokenizer.max_len - tokenizer.max_len_single_sentence)\n",
        "\n",
        "directory = \"/content/drive/My Drive/WJ/\"\n",
        "filename ='film_text.txt'\n",
        "\n",
        "block_size, directory, filename"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenizer max len 1024 tokenizer max len single sentence\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512, '/content/drive/My Drive/WJ/', 'film_text.txt')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngLHa70LdBdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c509af4f-7e1c-40e8-c256-5bf892ed4a90"
      },
      "source": [
        "cached_features_file = os.path.join(directory, \"gpt2\" + \"_\" + str(block_size) + \"_\" + filename)\n",
        "cached_features_file"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/WJ/gpt2_512_film_text.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrsbDi53dBf9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a43c3fc9-b958-4f91-b11f-d118516f6e28"
      },
      "source": [
        "overwrite_cache = False\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "if os.path.exists(cached_features_file) and not overwrite_cache: #if it already exists, don't overwrite if overwite_cache set to False\n",
        "      print('Loading featues from cached file')\n",
        "      logger.info(f\"Loading features from your cached file {cached_features_file}\") # report event\n",
        "\n",
        "      with open(cached_features_file, \"rb\") as cache:\n",
        "                self.examples = pickle.load(cache) #take binary data and deserialize to use \n",
        "                logger.debug(\"Loaded examples from cache\")\n",
        "\n",
        "else:\n",
        "      logger.info(f\"Creating features from file {filename} at {directory}\") #report event\n",
        "      print('Creating features from file')\n",
        "\n",
        "     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating features from file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4_kzpNdNDM8",
        "colab_type": "text"
      },
      "source": [
        "Let's  breakdown what is happening inside the second else:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pe-Oco3dBiW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f64ca473-b8fb-4264-d3ba-b2ebabb4a282"
      },
      "source": [
        "self.examples = []\n",
        "\n",
        "with open(FILE_PATH, encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "    print('read_file')\n",
        "    logger.debug(\"Succesfully read text from file\")\n",
        "\n",
        "#tokenize the text\n",
        "# convert_tokens_to_ids = Converts a token string (or a sequence of tokens) in a single integer id (or sequence of ids), using the vocabulary\n",
        "tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "read_file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTcZaAHoRD3-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c8b3b1bf-7d22-4da2-b188-f98806c0f079"
      },
      "source": [
        "len(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28066436"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX9yU-N3dBk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples = []\n",
        "\n",
        "for i in range(0, len(tokenized_text)-block_size +1, block_size):\n",
        "      examples.append(\n",
        "      tokenizer.build_inputs_with_special_tokens(\n",
        "          tokenized_text[i:i+block_size]\n",
        "      )\n",
        "  )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwdFFyY0dBnd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c7d24339-f3de-45c0-f4bc-1bce7f57b1de"
      },
      "source": [
        "len(examples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54817"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUns7DHwWC4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples[9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0wfHzLeFafS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b330dcdd-503f-4235-ba4e-7c38447b2434"
      },
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_7xJCMwOIki",
        "colab_type": "text"
      },
      "source": [
        "# Understanding model generate parameters\n",
        "\n",
        "\n",
        "* temperature:   It allows for more \"creativity\" the higher the temperature, the crazier the result. This means that the network is allowed to make sub-optimal predictions.\n",
        "\n",
        "* prefix: How you want your text to begin\n",
        "\n",
        "* length: Number of tokens to generate (default = 1023, which is also the maximum)\n",
        "\n",
        "* top_k: Limits the generated guesses to the top k guesses (default 0 will disable the behavior; if the generated output is super crazy you may want to set up top_k=40)\n",
        "* top_p: Nucleus sampling: limits the generated guesses based on a cumulative probability (gets good results on a dataset with top_p = 0.9)\n",
        "* truncate: Truncates the input text until it sees a pre-determined sequence (e.g. if truncate=<|endoftext|> the returned text will include everything before the first of those tokens) It may be useful to combine this with a smaller length if the input texts are short.\n",
        "* include_prefix: If using truncate  and include_prefix=False, the specified prefix won't be included in the returned text.\n",
        "\n",
        "* num_beams: Sometimes, greedy search can miss high probability words hidden behind a low probability word. If we set up num_beams say to n then at each step the model will keep track of n paths of high probability and will only choose the branch that in the end has the greatest probability even if at the beginning it didn't look like that was going to be the case (it was being greedy)\n",
        "\n",
        "Beam search is usually not very useful for open-ended generation where you don't have a specific lenght.\n",
        "\n",
        "\n",
        "* no_repeat_ngram_size : It ensures that there are not too many repetitions in the text. The most common n-gram penalty makes sure that no n-gram appears twice (no_repeat_ngram_size = 2)\n",
        "* num_return_sequences: If you want to choose the best output for your text you can set up num_return_sequences > 1 and then return which text you like the most (num_return_sequences <= num_beams)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htuU1u880Sjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKbF_0VEK_iW",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "\n",
        "* generate https://huggingface.co/transformers/v2.9.1/_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.generate\n",
        "\n",
        "* model generate https://huggingface.co/transformers/v2.9.1/main_classes/model.html\n",
        "\n",
        "* https://huggingface.co/blog/how-to-generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kPlIJ4dLGyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}