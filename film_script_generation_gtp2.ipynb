{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "film_script_generation_gtp2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1EYnGMObGkAEitlKNQrf2Zp07Axl7nPXX",
      "authorship_tag": "ABX9TyOZtTk5HSGVLbAV/CL0Qq5+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6f7ccfe2f66741c7880e89dbdfc5d742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4e3c916c84af47bda82ff6f40b061b66",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0823eb02797f43b4bfdb3af0307f8530",
              "IPY_MODEL_3d2fcca466144d6ca489cdbe6143d76e"
            ]
          }
        },
        "4e3c916c84af47bda82ff6f40b061b66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0823eb02797f43b4bfdb3af0307f8530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0a846cb1fbc74d7d9fdb0e584aef11b7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bf2bc7f103ee4cfbbf6938a9d66d4471"
          }
        },
        "3d2fcca466144d6ca489cdbe6143d76e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4251ea06d1624aab863a9bfb9346b73e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.04M/1.04M [00:01&lt;00:00, 609kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b11ab1f4280d44bb82a41df21a4a63a7"
          }
        },
        "0a846cb1fbc74d7d9fdb0e584aef11b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bf2bc7f103ee4cfbbf6938a9d66d4471": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4251ea06d1624aab863a9bfb9346b73e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b11ab1f4280d44bb82a41df21a4a63a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e56ebf5432c4a5da286141d456fca25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5c301fa791104badaa04b7017c657120",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4271e4c9f1b042b5bb3f4555218b875f",
              "IPY_MODEL_90eb9d9110784b21ac3cc80271eefd9c"
            ]
          }
        },
        "5c301fa791104badaa04b7017c657120": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4271e4c9f1b042b5bb3f4555218b875f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2cf18b87ecea4eee8d01d2460fdc49f6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_18ad1d933145433e8650527e93c11d94"
          }
        },
        "90eb9d9110784b21ac3cc80271eefd9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_08aeb45993c04ecdbf5b49b7c128831b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 1.08MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7d0f39c69aff4edda70e7b1d0e405113"
          }
        },
        "2cf18b87ecea4eee8d01d2460fdc49f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "18ad1d933145433e8650527e93c11d94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "08aeb45993c04ecdbf5b49b7c128831b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7d0f39c69aff4edda70e7b1d0e405113": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dbeda5fee73c4c168468e4b04601ec4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ce49e48bf88f4531b9d4cf1c2639e326",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_eed697f5fc4447bb821cd8e6303da0d9",
              "IPY_MODEL_8244faf9f24d4850ab9de3947df6a14a"
            ]
          }
        },
        "ce49e48bf88f4531b9d4cf1c2639e326": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eed697f5fc4447bb821cd8e6303da0d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e9c496d3149e4edcb85afb146ceb846d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 665,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 665,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9498fbd5ef14870826cd8b8666f3ecc"
          }
        },
        "8244faf9f24d4850ab9de3947df6a14a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6cc4ca98ca9b40b3a7c336772a3ed7d1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 665/665 [00:00&lt;00:00, 3.60kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4d8d6dd4b3824cae8024ee8af8accdb9"
          }
        },
        "e9c496d3149e4edcb85afb146ceb846d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9498fbd5ef14870826cd8b8666f3ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6cc4ca98ca9b40b3a7c336772a3ed7d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4d8d6dd4b3824cae8024ee8af8accdb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7205fdc1f6a642a3af3dd28b13a0d8a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_15cfbb9934204966927067355a4c11e2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1990635fda6e4e54a844d6347b4c1f3e",
              "IPY_MODEL_5824ed0b9fbf494cae2e1f6496bb1289"
            ]
          }
        },
        "15cfbb9934204966927067355a4c11e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1990635fda6e4e54a844d6347b4c1f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_16f12bca6b8046219e5de5fbdf9bc3f9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 548118077,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 548118077,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_27d12c06416045978157ca7124491423"
          }
        },
        "5824ed0b9fbf494cae2e1f6496bb1289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_699544fef4064248bec7181360343555",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [00:32&lt;00:00, 16.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41d34760dfa447608d3da6b6b6e089f7"
          }
        },
        "16f12bca6b8046219e5de5fbdf9bc3f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "27d12c06416045978157ca7124491423": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "699544fef4064248bec7181360343555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41d34760dfa447608d3da6b6b6e089f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizarci3/gtp2_film_generation/blob/master/film_script_generation_gtp2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zngvnu2UwJbf",
        "colab_type": "text"
      },
      "source": [
        "# Background\n",
        "\n",
        "Original article can be found [here](https://towardsdatascience.com/film-script-generation-with-gpt-2-58601b00d371)\n",
        "\n",
        "Repo [here](https://github.com/cdpierse/script_buddy_v2)\n",
        "\n",
        "The author used film scripts (~60 MB) of data scraped from the Internet Movie Script Database (IMSDB) in order to fine-tune a GTP2 to write a film script.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_StsRw6xxYJ2",
        "colab_type": "text"
      },
      "source": [
        "The author of this script only had ~1300 scripts to use, however, on averagea screenplay has 30,000 words. So the dataset has close to 40 million sequences of words.\n",
        "\n",
        "The author wanted the model to be able to generate entire sequences of scripts with mixed scripted elements in each sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x3MxXos4Z6c",
        "colab_type": "text"
      },
      "source": [
        "This fine-tuning is developed based on hugginface's example on fine-tuning dataset found in run_language_modeling.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Oj1u6_enWP1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "3917cab0-fb9a-4ab9-f788-a0958539d3e4"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=60859cab976680383b695afa7644e649c9e617d05a06ee16a6204b13dd1255d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.8 GB  |     Proc size: 111.2 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total     11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkEM-rEJ-xrA",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the data\n",
        "\n",
        "The script data is loaded into the model in batches were the data has already been tokenized for GPT-2. In the repo, the ScriptData class splits the entire dataset into tokenized blocks of tensors.\n",
        "\n",
        "Once the data has been properly prepared, these blocks are loaded in batches into a GPT-2 in a training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r-ZwZkNMMPf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "f1cca3b4-9dcb-40e7-be18-e9bddcecb6a2"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers #just doing a pip install transformers creates some sync problems"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-_io5t9k1\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-_io5t9k1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.18.5)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 41.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (0.16.0)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-3.0.2-cp36-none-any.whl size=868905 sha256=ee5bc93037e45af61aac13ec98d218e18b2edd6efab3675a93058edec0a403d3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6gzswvta/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=f211cb4be28e194d91108d400b75d110a377d0c3e854aabcf4df50004c82c838\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFh74RApLrem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import os\n",
        "import logging \n",
        "import pickle\n",
        "import logging\n",
        "import random"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBQQEEeTFt7v",
        "colab_type": "text"
      },
      "source": [
        "The gpt2-medium model used in this work has 12 layers ~345 million parameters and took ~6h to train (with 3 epochs).\n",
        "\n",
        "The first thing that needs to be done is to upload the model and tokenizer from the pre-trained transformers package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VxKJlVLf5zs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "fbba361b-c528-4fbf-d31e-23a484a1da13"
      },
      "source": [
        "device = 'cpu'\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "\n",
        "device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R2k5VtgFa-m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6f7ccfe2f66741c7880e89dbdfc5d742",
            "4e3c916c84af47bda82ff6f40b061b66",
            "0823eb02797f43b4bfdb3af0307f8530",
            "3d2fcca466144d6ca489cdbe6143d76e",
            "0a846cb1fbc74d7d9fdb0e584aef11b7",
            "bf2bc7f103ee4cfbbf6938a9d66d4471",
            "4251ea06d1624aab863a9bfb9346b73e",
            "b11ab1f4280d44bb82a41df21a4a63a7",
            "1e56ebf5432c4a5da286141d456fca25",
            "5c301fa791104badaa04b7017c657120",
            "4271e4c9f1b042b5bb3f4555218b875f",
            "90eb9d9110784b21ac3cc80271eefd9c",
            "2cf18b87ecea4eee8d01d2460fdc49f6",
            "18ad1d933145433e8650527e93c11d94",
            "08aeb45993c04ecdbf5b49b7c128831b",
            "7d0f39c69aff4edda70e7b1d0e405113",
            "dbeda5fee73c4c168468e4b04601ec4b",
            "ce49e48bf88f4531b9d4cf1c2639e326",
            "eed697f5fc4447bb821cd8e6303da0d9",
            "8244faf9f24d4850ab9de3947df6a14a",
            "e9c496d3149e4edcb85afb146ceb846d",
            "a9498fbd5ef14870826cd8b8666f3ecc",
            "6cc4ca98ca9b40b3a7c336772a3ed7d1",
            "4d8d6dd4b3824cae8024ee8af8accdb9",
            "7205fdc1f6a642a3af3dd28b13a0d8a5",
            "15cfbb9934204966927067355a4c11e2",
            "1990635fda6e4e54a844d6347b4c1f3e",
            "5824ed0b9fbf494cae2e1f6496bb1289",
            "16f12bca6b8046219e5de5fbdf9bc3f9",
            "27d12c06416045978157ca7124491423",
            "699544fef4064248bec7181360343555",
            "41d34760dfa447608d3da6b6b6e089f7"
          ]
        },
        "outputId": "15d53af0-260e-4b30-92a9-05793685d9d7"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.to(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f7ccfe2f66741c7880e89dbdfc5d742",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descriptâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e56ebf5432c4a5da286141d456fca25",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbeda5fee73c4c168468e4b04601ec4b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7205fdc1f6a642a3af3dd28b13a0d8a5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KINDLD6oGjr4",
        "colab_type": "text"
      },
      "source": [
        "* In order to fine tune this pre-trained model you need to create a training loop where you progressively load a batch of script sequences from the entire dataset.\n",
        "* Each batch is like a tokenized block of tensors from the data (done in ScriptData).\n",
        "* An important parameter to consider is the batch size. Large batch sizes can result in running out of GPU memory fast. To start, you can choose a batch of 1 and then test how much you can test it.\n",
        "\n",
        "* In this work his batch size was 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heJFj7VS_V-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from transformers import (\n",
        "    GPT2Config,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        ")\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7W7V0_cZciV",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the Data\n",
        "\n",
        "The following ScriptData class splits the dataset into tokenized blocks of tensors. These blocks will then be loaded in batches into the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzBvVN9g3wgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FILE_PATH = \"/content/drive/My Drive/WJ/film_text.txt\" # ~ 60 MB\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ScriptData(Dataset):\n",
        "\n",
        "  def __init__(\n",
        "      self, #instance of the class ScriptData\n",
        "      tokenizer: PreTrainedTokenizer,\n",
        "      file_path: str, \n",
        "      block_size = 512, # Fine-tuning item\n",
        "      overwrite_cache = False\n",
        "  ):\n",
        "\n",
        "      assert os.path.isfile(file_path) #assert raises an error if condition False\n",
        "\n",
        "      block_size = block_size - (tokenizer.max_len - tokenizer.max_len_single_sentence)\n",
        "\n",
        "      directory, filename = os.path.split(file_path)\n",
        "\n",
        "      #Create the path/filename for the cached file\n",
        "      # so that is stored in the same folder and it stores which block size we used\n",
        "    \n",
        "      cached_features_file = os.path.join(directory,\"gpt2\"+\"_\"+str(block_size)+\"_\"+filename)\n",
        "\n",
        "      #if the file already exists and if overwrite_cache is set to False don't overwrite\n",
        "      if os.path.exists(cached_features_file) and not overwrite_cache:\n",
        "        logger.info(f\"Loading features from cached file {cached_features_file}\")\n",
        "\n",
        "        with open(cached_features_file, 'rb') as cache:\n",
        "          self.examples = pickle.load(cache)\n",
        "          logger.debug(\"Loaded examples from cache\")\n",
        "\n",
        "      else:\n",
        "\n",
        "        logger.info(f\"Creating features from file {filename} at {directory}\")\n",
        "\n",
        "        self.examples = []\n",
        "\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "          text = f.read()\n",
        "          logger.debug(\"Succesfully read text from file\")\n",
        "\n",
        "        #convert_tokens_to_ids = Converts a token string (or a sequence of tokens) in a single integer id (or sequence of ids), using the vocabulary\n",
        "        tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
        "\n",
        "        #slice in steps of block_size the text\n",
        "        #append in examples\n",
        "\n",
        "        for i in range(0, len(tokenized_text) - block_size + 1, block_size):\n",
        "          self.examples.append(\n",
        "              tokenizer.build_inputs_with_special_tokens( #From Bert model: \n",
        "                  tokenized_text[i : i + block_size]\n",
        "              )\n",
        "          )\n",
        "\n",
        "        logger.info(f\"Saving features into cached file {cached_features_file}\")\n",
        "\n",
        "        # save it\n",
        "        with open(cached_features_file, \"wb\") as cache:\n",
        "          pickle.dump(self.examples, cache, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return torch.tensor(self.examples[item], dtype=torch.long)\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap9UBZprEuWu",
        "colab_type": "text"
      },
      "source": [
        "# Fine-tuning GPT-2: Training\n",
        "\n",
        "A GPU is necessary when training this model. We are using a dataset of film scripts that is about 60 MB to train. This text has been prepared by scrapping IMSDB (see in the repo the specifics of the scrapping)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAJXisafcko_",
        "colab_type": "text"
      },
      "source": [
        "We talked about how to fine tune the model (or optimize it on a custom dataset of tokenized text) you need to create a TRAINING LOOP WHERE YOU PROGRESSIVELY LOAD A BATCH OF SCRIPT SEQUENCES FROM THE DATASET.\n",
        "\n",
        "* Each batch (a batch of tokenized tensor) is run through the language model head as BOTH its intput and target labels.\n",
        "* From this step we return the loss and logits (i.e., prediction scores) to conduct the backward pass on the gradients.\n",
        "* Every X number of batches set up an evaluation step to generate a batch of text. This helps us understand how well the model is optimizing and being fine-tuned to the specific text.\n",
        "* Transformer's generate function provides a number of different decoding methods to get the best results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NCDHbQUZFnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_dir = '/content/drive/My Drive/WJ/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbX0rIZoeMwi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "c90264bd-0d9f-4c46-8307-d6d7a73492f8"
      },
      "source": [
        "dataset = ScriptData(tokenizer= tokenizer, file_path= FILE_PATH )\n",
        "script_loader = DataLoader(dataset,batch_size=4,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1355: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2QWPMoibZ4y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3c1a96fc-23e2-4ebd-a0af-8deb755c3cd4"
      },
      "source": [
        "type(script_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL3xEeRDZnz2",
        "colab_type": "text"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PufIi3UYUdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 1 #starting point, author used in the end a batch_size of 7\n",
        "EPOCHS = 1 # the author mentions he used in total 3 full epochs lasting ~6h for training\n",
        "LEARNING_RATE = 0.00002\n",
        "WARMUP_STEPS = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTcMfEkAZ2MS",
        "colab_type": "text"
      },
      "source": [
        "Start the optimizer, scheduler and set up the loss, batch counts to start at zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io3W0TKaYUgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.train()\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfGVZd6ZTLU0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e2dd4cb1-ebe1-476a-ed06-3f1a5547049c"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKXA1ZOaUBwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_perplexity(encoded_joke, input_stride):\n",
        "  lls = []\n",
        "\n",
        "\n",
        "  for i in range(1, encoded_joke.size(1), input_stride):\n",
        "    begin_loc = max(i + input_stride - encoded_joke.size(1), 0)\n",
        "    end_loc = i + input_stride\n",
        "\n",
        "    input_ids = encoded_joke[:,begin_loc:end_loc].to(device)\n",
        "\n",
        "    target_ids = input_ids.clone()\n",
        "    target_ids[:,:-input_stride] = -100\n",
        "\n",
        "    with torch.no_grad():\n",
        "      ppl_output = model(input_ids, labels=target_ids)\n",
        "      log_likelihood = ppl_output[0]*input_stride\n",
        "\n",
        "    lls.append(log_likelihood)\n",
        "\n",
        "  perplex = torch.exp(torch.stack(lls).sum()/i)\n",
        "\n",
        "  return(perplex.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjD7JoJ87jbt",
        "colab_type": "text"
      },
      "source": [
        "## Just for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE1xdpHCYUko",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "7570dae8-e7de-4ced-b111-7243daa0fa46"
      },
      "source": [
        "script_count = 0\n",
        "sum_loss = 0.0\n",
        "batch_count = 0\n",
        "\n",
        "# input text: You can use input_ids or bos_token_id to start your text generation\n",
        "# bos_token_id should be 1 positive int (in the setup we have below, it initializes with a random word)\n",
        "# in model generate choose how you want your text to begin\n",
        "\n",
        "test_text = \"Once upon a time there was a dog\"\n",
        "tokenized_test = torch.tensor(tokenizer.encode(test_text)).unsqueeze(0).to(device)\n",
        "\n",
        "# or\n",
        "bos_token_id_gen = random.randint(1,30000) \n",
        "\n",
        "\n",
        "# for perplexity\n",
        "stride = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"EPOCH {epoch} started\"+'='*30)\n",
        "\n",
        "  j = 0\n",
        "  nsamples = 199\n",
        "\n",
        "  for idx, script in enumerate(script_loader):\n",
        "\n",
        "    if j>nsamples:\n",
        "      break\n",
        "\n",
        "    else:\n",
        "      #print(idx, script, script[0].shape) #512 was what we used as block size in ScriptData\n",
        "\n",
        "      outputs = model(script.to(device), labels=script.to(device))\n",
        "\n",
        "      loss, logits = outputs[:2] # language modeling loss and prediction scores of the language modeling head \n",
        "      loss.backward()\n",
        "\n",
        "      sum_loss = sum_loss + loss.detach().data\n",
        "      script_count = script_count + 1\n",
        "      #print('Sum loss', sum_loss, 'script_count', script_count)\n",
        "      \n",
        "      #once we have loaded enough scripts == batch_size\n",
        "\n",
        "      j = j + 1 \n",
        "\n",
        "      if script_count == BATCH_SIZE: #7 in original text\n",
        "        #print('script_count equal to batch_size')\n",
        "        script_count = 0 # re-start script counter\n",
        "        batch_count = batch_count + 1 # how many full batches we have\n",
        "        #print('batch_count', batch_count)\n",
        "        \n",
        "        optimizer.step() # perform an optimization step\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad() # clear the gradients from the last step, PyTorch accumulates the gradients so before starting to propagate we need to set them to zero\n",
        "        model.zero_grad()\n",
        "\n",
        "\n",
        "        ## As opposed to the jokes generation script, this script generates text\n",
        "        ## Apparently, every X number of batches to see how it is doing\n",
        "       \n",
        "        if batch_count == 200:\n",
        "            model.eval()  \n",
        "            print(f\"sum loss {sum_loss}\")\n",
        "\n",
        "            ## see https://huggingface.co/blog/how-to-generate\n",
        "           \n",
        "            sample_outputs = model.generate( #function added since version 2.4\n",
        "                                    #bos_token_id=bos_token_id_gen, # Beginning of sentence token if no prompt is provided. The sequence used as a prompt for the generation. If `None` the method initializes  it as an empty `torch.LongTensor` of shape `(1,)`. \n",
        "                                    input_ids = tokenized_test, #(optional) tf.Tensor of dtype=tf.int32 of shape (batch_size, sequence_length) The sequence used as a prompt for the generation. If None the method initializes it as an empty tf.Tensor of shape (1,).\n",
        "                                    do_sample=True, #If set to `False` greedy decoding is used. Otherwise sampling is used. In its basic form, in sampling you randomly choose the next token  \n",
        "                                    top_k=50, # The number of highest probability vocabulary tokens to keep for top-k-filtering. Must be between 0 and 1. Default to 50.  \n",
        "                                    max_length = 1000, # The max length of the sequence to be generated.  Between `min_length` and infinity. Default to 20.\n",
        "                                    top_p=0.95, #The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.  \n",
        "                                    num_return_sequences=1 #The number of independently computed returned sequences. If you want to choose between different options set > 1. Default to 1.\n",
        "                                )\n",
        "\n",
        "            print(\"Output:\\n\" + 100 * '-')\n",
        "            #print('bos_token_id_gen', bos_token_id_gen)\n",
        "            for i, sample_output in enumerate(sample_outputs):\n",
        "\n",
        "                  ppl = calculate_perplexity(sample_outputs, stride)\n",
        "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "                  print('Perplexity', ppl)\n",
        "            \n",
        "            batch_count = 0\n",
        "            sum_loss = 0.0\n",
        "            model.train()\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 0 started==============================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "sum loss 442.0477294921875\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: Once upon a time there was a dog who lived in the house with a little brother. When we began to run away to the house a boy saw a little dog that he knew. He thought it was a black cat and he went away, crying.\n",
            "\n",
            "A man called me and told me, \"I don't know the names of the men who killed the dog. It was in the neighbourhood, so I am very sorry.\"\n",
            "\n",
            "A woman said, \"There were two brothers living, and we ran after them. They were afraid of me because I am black, so they followed them and killed our little dog.\"\n",
            "\n",
            "At first I did not speak to them and they did not stop us. I found out later that they were trying to escape to the south.\n",
            "\n",
            "On Christmas I went there to have a tea. Then when I came out there there was a lady of the same family, who had a dog, and she found a small dog with it, and she was crying.\n",
            "\n",
            "On the Christmas eve of Christmas there was another boy, who said that he had seen a dog and that a woman saw him. He gave me a letter that the lady of the same family had sent, telling me that he had been afraid of me.\n",
            "\n",
            "The child who was afraid of me also came out on New Year's Eve, which is during the year when I work and this woman came out on New Year's Eve. And this child was dressed like her, and she said, \"Well, there she is. I am in the neighbourhood, and I am looking out over the street, when I see her I am very afraid.\"\n",
            "\n",
            "I heard a noise when I went to check the house and at that time I found this little girl on the pavement, but I didn't see her. I did see her and I did not feel anything. There were all these people, but nobody was there. It was very sad for her, she said, because now she is dead.\n",
            "\n",
            "And then I saw what was going on in the neighbourhood. I saw a lot of people there. The neighbourhood has been ruined for a hundred years now. Now the children of this people were running to get away. People are looking out over the road, all these people are afraid. There was a man, who came out of nowhere, and said he was frightened. He went down to a church and said he saw people running away from the church. Then he was walking with his dog, and it began to run away. A small dog was running in the front of the church, and I couldn't move it, so I left it and ran away.\n",
            "\n",
            "Then one of the people who came out was looking at a big dog in the front of the church. Then he saw two men, and they ran at him. Then he was running around, but I did not see him, and so he disappeared out of the crowd of people, who were running towards the church, and that was what I saw, and then I saw where he was going.\n",
            "\n",
            "When my brother and I got out of the house in the afternoon I said to the women, \"What you saw is my brother and me, and you don't know what's up here. But we are so far away from home, and we are all going to be lost. If we lose it here, we all will die.\"\n",
            "\n",
            "I went back to my house and went to the church at the church on New Year's Eve, and found there a lot of men gone away. But some were dressed as a beggar, and other women dressed as nuns, and some had their headscarves and they took part in the service. Some of them wore masks, one wearing a white costume, one carrying a golden amulet and the other wearing a purple one.\n",
            "\n",
            "I ran into my brother and went back to his house. They told me they were going to bury the body and bury it. They said they could not, that there might be some bodies buried, and they had not received the money from a friend and wanted the body. They told me I could not bury the body for fear it might be put to death. And they were not satisfied, and after talking to me, I said to the nun who had buried it, \"If it was her, we could ask for all the money we could get from the friend, and if that friend wants money, she might come with us to collect it.\"\n",
            "\n",
            "They said they were going to have the bones for burial when I returned. I said to the nun who had the body, \"I cannot make a deal between us.\"\n",
            "\n",
            "Now one of the nuns who had buried the body that I saw at the cemetery said to the nun that she could not get money from the friend so she asked me what I wanted. I said it would be enough to pay for\n",
            "Perplexity 11.607949256896973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmMCBB_JfvzS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3c88a3a6-9ba9-4c98-f192-61c644ae0f40"
      },
      "source": [
        "ppl = calculate_perplexity(sample_outputs, 300)\n",
        "ppl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24.807331085205078"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c45d1hzgAQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eedk2u18bFxq",
        "colab_type": "text"
      },
      "source": [
        "# Breaking down preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pui05vrJa0Tn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FILE_PATH = \"/content/drive/My Drive/WJ/film_text.txt\" # ~ 60 MB\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
        "\n",
        "model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qEsd7OnaxhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert os.path.isfile(FILE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55UX9iYRYOfB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3b61922a-463c-48a3-de45-0d0ea34f921d"
      },
      "source": [
        "tokenizer.max_len, tokenizer.max_len_single_sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1024, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFZOxrpQdBag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3d10c416-b11a-42bc-8041-a250da8f924d"
      },
      "source": [
        "block_size = 512 # I am assuming this is the size of the block that will be loaded into the training loop\n",
        "\n",
        "print('tokenizer max len', tokenizer.max_len, 'tokenizer max len single sentence')\n",
        "\n",
        "block_size = block_size - (tokenizer.max_len - tokenizer.max_len_single_sentence)\n",
        "\n",
        "directory = \"/content/drive/My Drive/WJ/\"\n",
        "filename ='film_text.txt'\n",
        "\n",
        "block_size, directory, filename"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenizer max len 1024 tokenizer max len single sentence\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512, '/content/drive/My Drive/WJ/', 'film_text.txt')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngLHa70LdBdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c509af4f-7e1c-40e8-c256-5bf892ed4a90"
      },
      "source": [
        "cached_features_file = os.path.join(directory, \"gpt2\" + \"_\" + str(block_size) + \"_\" + filename)\n",
        "cached_features_file"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/WJ/gpt2_512_film_text.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrsbDi53dBf9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a43c3fc9-b958-4f91-b11f-d118516f6e28"
      },
      "source": [
        "overwrite_cache = False\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "if os.path.exists(cached_features_file) and not overwrite_cache: #if it already exists, don't overwrite if overwite_cache set to False\n",
        "      print('Loading featues from cached file')\n",
        "      logger.info(f\"Loading features from your cached file {cached_features_file}\") # report event\n",
        "\n",
        "      with open(cached_features_file, \"rb\") as cache:\n",
        "                self.examples = pickle.load(cache) #take binary data and deserialize to use \n",
        "                logger.debug(\"Loaded examples from cache\")\n",
        "\n",
        "else:\n",
        "      logger.info(f\"Creating features from file {filename} at {directory}\") #report event\n",
        "      print('Creating features from file')\n",
        "\n",
        "     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating features from file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4_kzpNdNDM8",
        "colab_type": "text"
      },
      "source": [
        "Let's  breakdown what is happening inside the second else:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pe-Oco3dBiW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f64ca473-b8fb-4264-d3ba-b2ebabb4a282"
      },
      "source": [
        "self.examples = []\n",
        "\n",
        "with open(FILE_PATH, encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "    print('read_file')\n",
        "    logger.debug(\"Succesfully read text from file\")\n",
        "\n",
        "#tokenize the text\n",
        "# convert_tokens_to_ids = Converts a token string (or a sequence of tokens) in a single integer id (or sequence of ids), using the vocabulary\n",
        "tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "read_file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTcZaAHoRD3-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c8b3b1bf-7d22-4da2-b188-f98806c0f079"
      },
      "source": [
        "len(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28066436"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX9yU-N3dBk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples = []\n",
        "\n",
        "for i in range(0, len(tokenized_text)-block_size +1, block_size):\n",
        "      examples.append(\n",
        "      tokenizer.build_inputs_with_special_tokens(\n",
        "          tokenized_text[i:i+block_size]\n",
        "      )\n",
        "  )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwdFFyY0dBnd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c7d24339-f3de-45c0-f4bc-1bce7f57b1de"
      },
      "source": [
        "len(examples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54817"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUns7DHwWC4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples[9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0wfHzLeFafS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b330dcdd-503f-4235-ba4e-7c38447b2434"
      },
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_7xJCMwOIki",
        "colab_type": "text"
      },
      "source": [
        "# Understanding model generate parameters\n",
        "\n",
        "\n",
        "* temperature:   It allows for more \"creativity\" the higher the temperature, the crazier the result. This means that the network is allowed to make sub-optimal predictions.\n",
        "\n",
        "* prefix: How you want your text to begin\n",
        "\n",
        "* length: Number of tokens to generate (default = 1023, which is also the maximum)\n",
        "\n",
        "* top_k: Limits the generated guesses to the top k guesses (default 0 will disable the behavior; if the generated output is super crazy you may want to set up top_k=40)\n",
        "* top_p: Nucleus sampling: limits the generated guesses based on a cumulative probability (gets good results on a dataset with top_p = 0.9)\n",
        "* truncate: Truncates the input text until it sees a pre-determined sequence (e.g. if truncate=<|endoftext|> the returned text will include everything before the first of those tokens) It may be useful to combine this with a smaller length if the input texts are short.\n",
        "* include_prefix: If using truncate  and include_prefix=False, the specified prefix won't be included in the returned text.\n",
        "\n",
        "* num_beams: Sometimes, greedy search can miss high probability words hidden behind a low probability word. If we set up num_beams say to n then at each step the model will keep track of n paths of high probability and will only choose the branch that in the end has the greatest probability even if at the beginning it didn't look like that was going to be the case (it was being greedy)\n",
        "\n",
        "Beam search is usually not very useful for open-ended generation where you don't have a specific lenght.\n",
        "\n",
        "\n",
        "* no_repeat_ngram_size : It ensures that there are not too many repetitions in the text. The most common n-gram penalty makes sure that no n-gram appears twice (no_repeat_ngram_size = 2)\n",
        "* num_return_sequences: If you want to choose the best output for your text you can set up num_return_sequences > 1 and then return which text you like the most (num_return_sequences <= num_beams)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKbF_0VEK_iW",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "\n",
        "* generate https://huggingface.co/transformers/v2.9.1/_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.generate\n",
        "\n",
        "* model generate https://huggingface.co/transformers/v2.9.1/main_classes/model.html\n",
        "\n",
        "* https://huggingface.co/blog/how-to-generate"
      ]
    }
  ]
}